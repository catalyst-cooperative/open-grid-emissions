{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 930 lag\n",
    "\n",
    "Currently we assume that PJM and CISO report start-of-hour to EIA-930 for both generation and interchange, while all other BAs report end-of-hour for both. \n",
    "\n",
    "This is based on analysis comparing EIA-930 to ISO-reported data, so the analysis did not consider small western BAs. \n",
    "\n",
    "Manual inspection of interchange data between CISO and nearby BAs indicates that CISO is not lagged relative to these BAs, so one of two things is happening: \n",
    "\n",
    "1) CISO interchange data is end-of-hour \n",
    "\n",
    "2) BAs surrounding CISO report start-of-hour for interchange, like CISO, and maybe also for generation \n",
    "\n",
    "### Plan \n",
    "\n",
    "1) calculate correlation between CEMS and 930 generation time series in each BA; run lagged correlation (1h in each direction), check which is more correlated \n",
    "\n",
    "2) repeat for correlation between interchange and interchange*(-1) for neighboring BAs\n",
    "\n",
    "3) shift per-BA generation and interchange data as indicated by the above\n",
    "\n",
    "4) repeat steps 1&2 to ensure that we are now correct\n",
    "\n",
    "\n",
    "### Algorithm for figuring out how much to lag by \n",
    "\n",
    "#### Generation: \n",
    "\n",
    "What's the lag consistent over checks? use that. \n",
    "\n",
    "#### Interchange: \n",
    "\n",
    "* Is non-zero lag consistent across my connections? if not, assume I'm fine and the other one is the issue. \n",
    "* Is non-zero lag the same across my connections? \n",
    "\n",
    "\n",
    "### Checks \n",
    "* Do we have the same plan using different years? \n",
    "* Do we have the same plan using April - October (inclusive) vs using December - February (inclusive)? (with / wo DST)\n",
    "* Is demand (D) correlated with generation (NG) without lag in 930 data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Tell python where to look for modules. \n",
    "# Depending on how your jupyter handles working directories, this may not be needed.\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from src.download_data import download_chalendar_files\n",
    "from src.eia930 import reformat_chalendar, manual_930_adjust\n",
    "from src.load_data import PATH_TO_LOCAL_REPO, data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if not exists\n",
    "download_chalendar_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(f\"{data_folder()}/downloads/eia930/chalendar/EBA_raw.csv\",index_col=0, parse_dates=True)\n",
    "fixed = manual_930_adjust(raw)\n",
    "fixed.to_csv(f\"{data_folder()}/outputs/EBA_adjusted_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#eia930 = pd.read_csv(\"../data/eia930/chalendar/EBA_rolling.csv\",index_col=0, parse_dates=True)\n",
    "eia930 = pd.read_csv(f\"{data_folder()}/outputs/EBA_adjusted_raw.csv\",index_col=0, parse_dates=True)\n",
    "eia930 = eia930[eia930.index>\"2018-07-01T00:00\"] # limit to after gen was reported by fuel type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia930_raw = reformat_chalendar(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia930 = reformat_chalendar(eia930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some bad values missed in rolling filter\n",
    "eia930.loc[(eia930.BA==\"SEC\") & (eia930.generation > 50000), \"generation\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_meta = pd.read_csv(\"../../data/outputs/2020/plant_static_attributes_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = [f\"{data_folder()}/outputs/2019/cems_2019.csv\", f\"{data_folder()}/outputs/2020/cems_2020.csv\"]\n",
    "\n",
    "# Load files\n",
    "# Aggregate by BA during loading to cut down on space\n",
    "cems = pd.DataFrame()\n",
    "for y in files: \n",
    "    print(f\"loading {y}\")\n",
    "    c = pd.read_csv(y, index_col=0, parse_dates=['datetime_utc'])\n",
    "    c = c.rename(columns={\"datetime_utc\":\"datetime_utc\"})\n",
    "    c = c.merge(plant_meta[['plant_id_eia', 'plant_primary_fuel', 'ba_code']], how='left', left_index=True, right_on='plant_id_eia')\n",
    "    # exclude solar power for CEMS, since we're just going to look at COL + OIL + NG in the 930 data\n",
    "    c = c[c[\"plant_primary_fuel\"] != \"SUN\"]\n",
    "    print(\"Aggregating\")\n",
    "    cems_aggregated = c.groupby([\"datetime_utc\",\"ba_code\"]).sum()[\"net_generation_mwh\"].reset_index()\n",
    "    cems = pd.concat([cems, cems_aggregated])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case when loading CEMS data: for some BAs, some plants are reported in either 2019 and 2020 files, so need to groupby again to catch those\n",
    "cems = cems.groupby([\"datetime_utc\",\"ba_code\"]).sum()[\"net_generation_mwh\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for fossil fuels, sum by BA\n",
    "eia930 = eia930[eia930.fuel.isin([\"COL\",\"NG\",\"OIL\"])]\n",
    "eia930 = eia930.groupby([\"datetime_utc\",\"BA\"]).sum()[\"generation\"].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas = set(eia930.BA.unique())\n",
    "bas.intersection_update(set(cems.ba_code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shared BAs: {len(bas)} out of {len(eia930.BA.unique())} 930 BAs and {len(cems.ba_code.unique())} CEMS BAs.\")\n",
    "\n",
    "missing_cems = set(eia930.BA.unique()).difference(set(cems.ba_code.unique()))\n",
    "missing_930 = set(cems.ba_code.unique()).difference(set(eia930.BA.unique()))\n",
    "print(f\"930 BAs missing in CEMS: {missing_cems}\")\n",
    "print(f\"CEMS missing 930: {missing_930}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cor(cems, eia930):\n",
    "    cems = cems.pivot(columns=\"ba_code\", index=\"datetime_utc\", values=\"net_generation_mwh\")\n",
    "    eia930 = eia930.pivot(columns=\"BA\", index=\"datetime_utc\", values=\"generation\")\n",
    "\n",
    "    bas = set(cems.columns).intersection(set(eia930.columns))\n",
    "\n",
    "    correlations = pd.DataFrame(index=bas, columns=range(-12,12), dtype=float)\n",
    "\n",
    "    for ba in correlations.index:\n",
    "        for lag in correlations.columns:\n",
    "            # prepare 930: select BA \n",
    "            #eia = eia930[eia930.BA==ba][\"generation\"]\n",
    "            # prepare CEMS: select BA\n",
    "            #c = cems[cems.ba_code==ba][\"net_generation_mwh\"]\n",
    "            # calculate \n",
    "            correlations.loc[ba,lag] = cems[ba]\\\n",
    "                .corr(eia930[ba].shift(lag))\n",
    "\n",
    "    best = correlations.apply(lambda s: s.index[s.argmax()], axis=1).rename(\"best\")\n",
    "\n",
    "    correlations = pd.concat([best, correlations], axis='columns')\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_best_cor(cems[(cems.datetime_utc.dt.month>=11)|(cems.datetime_utc.dt.month<=2)],\n",
    "#        eia930[(eia930.datetime_utc.dt.month>=11)|(eia930.datetime_utc.dt.month<=2)]).loc[\"WALC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate correlations using different subsets of 930 data \n",
    "\n",
    "cems_930_cors = pd.concat([find_best_cor(cems, eia930).best.rename(\"all_years\"),\\\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2019],eia930[eia930.datetime_utc.dt.year==2019]).best.rename(\"2019\"),\n",
    "    find_best_cor(cems[cems.datetime_utc.dt.year==2020],eia930[eia930.datetime_utc.dt.year==2020]).best.rename(\"2020\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=4)&(cems.datetime_utc.dt.month<=9)],\n",
    "        eia930[(eia930.datetime_utc.dt.month>=4)&(eia930.datetime_utc.dt.month<=9)]).best.rename(\"daylight time\"),\n",
    "    find_best_cor(cems[(cems.datetime_utc.dt.month>=11)|(cems.datetime_utc.dt.month<=2)],\n",
    "        eia930[(eia930.datetime_utc.dt.month>=11)|(eia930.datetime_utc.dt.month<=2)]).best.rename(\"standard time\")],\n",
    "    axis='columns')\n",
    "\n",
    "#cems_930_cors.to_csv(\"../data/outputs/cems_eia930_cor_lags.csv\")\n",
    "cems_930_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia930_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"SC\"\n",
    "\n",
    "to_plot_930 = eia930_raw[eia930_raw.BA==ba].groupby(\"datetime_utc\").sum()\n",
    "\n",
    "print(f\"correlations for {ba}\")\n",
    "print(cems_930_cors.loc[ba])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=cems[cems.ba_code==ba].datetime_utc, y=cems[cems.ba_code==ba].net_generation_mwh, name=\"CEMS\"))\n",
    "fig.add_trace(go.Scatter(x=to_plot_930.index, y=to_plot_930.generation, name=\"EIA 930 (before adjustment)\"))\n",
    "fig.update_layout(\n",
    "    title=ba,\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interchange = pd.read_csv(\"../data/eia930/chalendar/EBA_rolling.csv\",index_col=0, parse_dates=True)\n",
    "interchange = pd.read_csv(\"../data/outputs/EBA_adjusted_raw.csv\",index_col=0, parse_dates=True)\n",
    "interchange = interchange[interchange.index>\"2018-07-01T00:00\"] # limit to after gen was reported by fuel type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas930 = {re.split(r\"[-.]\",c)[1] for c in interchange.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchange.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching BAs to interchange_cors dict\n",
    "# optionally, write markdown to {file}.md and csvs at {file}_{ba}.csv\n",
    "def interchange_cor(interchange, interchange_cors:dict={}, file=\"\", name:str=\"cors\"):\n",
    "    # Delete file\n",
    "    if file != \"\":\n",
    "        hs = open(file+\".md\",\"w\")\n",
    "        hs.write(\"\\n\\n\")\n",
    "        hs.close() \n",
    "\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [c for c in interchange.columns \\\n",
    "            if re.split(r\"[-.]\",c)[1]==ba \\\n",
    "                and re.split(r\"[-.]\",c)[2]!=\"ALL\"]\n",
    "        other_bas = [re.split(r\"[-.]\",c)[2] for c in other_cols]\n",
    "        #print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12,12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba,lag] = abs(interchange[this_way]\\\n",
    "                    .corr(-1*interchange[other_way].shift(lag)))\n",
    "        \n",
    "        # where is correlation the best?\n",
    "        out = pd.concat([out, out.apply(lambda s: s.index[s.argmax()], axis=1).rename(\"best\")], axis='columns')\n",
    "\n",
    "        if file != \"\":\n",
    "            # add new lines for proper markdown syntax\n",
    "            hs = open(file+\".md\",\"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close() \n",
    "\n",
    "            out.to_markdown(file+\".md\",mode=\"a\")\n",
    "\n",
    "            out.to_csv(f\"{file}_{ba}\"+\".csv\")\n",
    "\n",
    "        interchange_cors[ba] = pd.concat([interchange_cors.get(ba, pd.DataFrame()), out.best.rename(name)], axis='columns')\n",
    "\n",
    "    return interchange_cors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cors = interchange_cor(interchange, interchange_cors={}, name=\"all_years\")\n",
    "int_cors = interchange_cor(interchange[\"2019-01-01T00:00\":\"2019-12-30T00:00\"], int_cors, name=\"2019\")\n",
    "int_cors = interchange_cor(interchange[\"2020-01-01T00:00\":\"2020-12-30T00:00\"], int_cors, name=\"2020\")\n",
    "int_cors = interchange_cor(interchange[(interchange.index.month >= 4)&(interchange.index.month <=9)], int_cors, name=\"daylight savings\")\n",
    "int_cors = interchange_cor(interchange[(interchange.index.month >= 11)|(interchange.index.month <=2)], int_cors, name=\"standard time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cors[\"PJM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cors.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../data/outputs/interchange_correlations/summary_adjusted.md\"\n",
    "hs = open(file,\"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close() \n",
    "\n",
    "for (ba,out) in int_cors.items():\n",
    "\n",
    "    # add new lines for proper markdown syntax\n",
    "            hs = open(file,\"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close() \n",
    "\n",
    "            out.to_markdown(file,mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"PJM\"\n",
    "\n",
    "# find cols of mappings in both directions \n",
    "other_cols = [c for c in interchange.columns \\\n",
    "    if re.split(r\"[-.]\",c)[1]==ba \\\n",
    "        and re.split(r\"[-.]\",c)[2]!=\"ALL\"]\n",
    "other_bas = [re.split(r\"[-.]\",c)[2] for c in other_cols]\n",
    "\n",
    "these_cols = [f\"EBA.{o_ba}-{ba}.ID.H\" for o_ba in other_bas]\n",
    "\n",
    "# make long version with just cols of interest, adding BA column and to/from column\n",
    "toplot = pd.DataFrame()\n",
    "for i in range(len(other_bas)): \n",
    "    to_add = (interchange[other_cols[i]]).rename(\"interchange\").to_frame()\n",
    "    to_add[\"source\"] = ba\n",
    "    to_add[\"BA\"] = other_bas[i]\n",
    "\n",
    "    to_add_2 = (interchange[these_cols[i]]*(-1)).rename(\"interchange\").to_frame()\n",
    "    to_add_2[\"source\"] = \"other BA\"\n",
    "    to_add_2[\"BA\"] = other_bas[i]\n",
    "\n",
    "    toplot = pd.concat([toplot, to_add, to_add_2], axis='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(toplot, x=toplot.index, y=\"interchange\", facet_col=\"BA\", facet_col_wrap=2, color=\"source\")\n",
    "fig.update_layout(\n",
    "    title=f\"Interchange from {ba}\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Source for<br>interchange data\"\n",
    ")\n",
    "fig.for_each_annotation(lambda a: a.update(text=\"Other \"+a.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=\"PJM\"\n",
    "second=\"MISO\"\n",
    "\n",
    "fig = px.line(interchange, x=interchange.index, y=[f\"EBA.{first}-{second}.ID.H\",f\"EBA.{second}-{first}.ID.H\", f\"EBA.{first}-ALL.TI.H\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{first}/{second} interchange\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Interchange\",\n",
    "    legend_title=\"Series\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = \"CFE\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=interchange.index, \n",
    "    y=interchange[f\"EBA.{ba}-ALL.D.H\"]-interchange[f\"EBA.{ba}-ALL.NG.H\"]))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{ba} demand - generation\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Demand - generation\",\n",
    "    legend_title=\"Series\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign issues across interchange data\n",
    "\n",
    "Most interchanges should be negatively correlated with the interchange coming the other way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a df where columns are interchange data, add best correlation between matching BAs to interchange_cors dict\n",
    "# optionally, write markdown to {file}.md and csvs at {file}_{ba}.csv\n",
    "def interchange_sign(interchange, i_sign:dict={}, file=\"\", name:str=\"cors\"):\n",
    "    for ba in bas930:\n",
    "        print(ba, end=\"...\")\n",
    "        other_cols = [c for c in interchange.columns \\\n",
    "            if re.split(r\"[-.]\",c)[1]==ba \\\n",
    "                and re.split(r\"[-.]\",c)[2]!=\"ALL\"]\n",
    "        other_bas = [re.split(r\"[-.]\",c)[2] for c in other_cols]\n",
    "        #print(f\"{ba} connects to {other_bas}\")\n",
    "\n",
    "        out = pd.DataFrame(index=other_bas, columns=range(-12,12), dtype=float)\n",
    "        for o_ba in out.index:\n",
    "            this_way = f\"EBA.{o_ba}-{ba}.ID.H\"\n",
    "            other_way = f\"EBA.{ba}-{o_ba}.ID.H\"\n",
    "            for lag in out.columns:\n",
    "                out.loc[o_ba,lag] = interchange[this_way]\\\n",
    "                    .corr(-1*interchange[other_way].shift(lag))\n",
    "        \n",
    "        # where is correlation the best?\n",
    "        out = out.apply(lambda s: s.iloc[abs(s).argmax()], axis=1)\n",
    "\n",
    "        i_sign[ba] = pd.concat([i_sign.get(ba, pd.DataFrame()), out.rename(name)], axis='columns')\n",
    "\n",
    "    return i_sign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sign = interchange_sign(interchange, {}, name=\"all_years\")\n",
    "int_sign = interchange_sign(interchange[\"2019-01-01T00:00\":\"2019-12-30T00:00\"], int_sign, name=\"2019\")\n",
    "int_sign = interchange_sign(interchange[\"2020-01-01T00:00\":\"2020-12-30T00:00\"], int_sign, name=\"2020\")\n",
    "int_sign = interchange_sign(interchange[(interchange.index.month >= 4)&(interchange.index.month <=9)], int_sign, name=\"daylight savings\")\n",
    "int_sign = interchange_sign(interchange[(interchange.index.month >= 11)|(interchange.index.month <=2)], int_sign, name=\"standard time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../data/outputs/interchange_cors_sign.md\"\n",
    "hs = open(file,\"w\")\n",
    "hs.write(\"\\n\\n\")\n",
    "hs.close() \n",
    "\n",
    "for (ba,out) in int_sign.items():\n",
    "\n",
    "    # add new lines for proper markdown syntax\n",
    "            hs = open(file,\"a\")\n",
    "            hs.write(f\"\\n\\n# {ba}\\n\\n\")\n",
    "            hs.close() \n",
    "\n",
    "            out.to_markdown(file,mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is D = G - I better lagged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = pd.DataFrame(index=bas930, columns=range(-2,3), dtype=float)\n",
    "\n",
    "for ba in bas930:\n",
    "    for lag in differences.columns:\n",
    "        dif = interchange[f\"EBA.{ba}-ALL.NG.H\"]-\\\n",
    "            interchange[f\"EBA.{ba}-ALL.D.H\"]-\\\n",
    "            interchange[f\"EBA.{ba}-ALL.TI.H\"].shift(lag)\n",
    "        differences.loc[ba,lag] = dif.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = pd.concat([differences.apply(lambda s: s.index[s.argmin()], axis=1).rename(\"best\"),\n",
    "    differences], axis='columns')\n",
    "\n",
    "differences.to_markdown(\"../data/outputs/lagged_differences.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"../data/eia930/chalendar/EBA_rolling.csv\",index_col=0, parse_dates=True)\n",
    "fixed = manual_930_adjust(raw)\n",
    "fixed.to_csv(\"../data/outputs/EBA_adjusted_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = manual_930_adjust(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed.to_csv(\"../data/outputs/EBA_adjusted_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dst = raw.index.tz_convert(\"US/Eastern\").to_series().apply(lambda s: s.utcoffset()) == timedelta(hours=-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjm_offset = [timedelta(hours=-3) if is_d else timedelta(hours=-4) for is_d in is_dst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pjm_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('hourly_egrid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65c02dfd2dc2ef471c0b5088763a28c1faaa7cad28937ca42fadf51e669fd8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
